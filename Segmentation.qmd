---
title: "Customer Segmentation"
author: "MIDHUN MADHU"
format: html
editor: visual
---

# [Market Segmentation Report]{.underline}

```{r echo=FALSE,message=FALSE,warning=FALSE}
library(rattle)   # Access the weather dataset and utilities.
library(magrittr) # Utilise %>% and %<>% pipeline operators.
library(readxl, quietly=TRUE)

 crs$dataset <- read_excel("C:/Users/midhu/OneDrive/Desktop/R Prgm/DemoKTC.xlsx", guess_max=1e4)

 crs$dataset
```

```{r echo=FALSE,message=FALSE,warning=FALSE }
library(rattle)   # Access the weather dataset and utilities.
library(magrittr) # Utilise %>% and %<>% pipeline operators.



building <- TRUE
scoring  <- ! building



crv$seed <- 42 
```

## 1.Introduction

**The KTC Company** would like to segment their customers based on their characteristics.The company has information regarding the customers for segmentation.The company would like to group customers as per their requirements and characteristics based on the data the entity have regarding customers.

## 2.Descriptive Mining

### 2.1 Exploratory Data Analysis (Data Exploration)

We have information regarding 30 customers of KTC Company.The Company has details of their age,income,gender,marital status,dependents(number of children),financial status(car loan,mortgage loan).

```{r echo=FALSE,message=FALSE,warning=FALSE}
# The following variable selections have been noted.

crs$input     <- c("Age", "Female", "Income", "Married",
                   "Children", "Loan")

crs$numeric   <- c("Age", "Female", "Income", "Married",
                   "Children", "Loan")

crs$categoric <- NULL

crs$target    <- "Mortgage"
crs$risk      <- NULL
crs$ident     <- NULL
crs$ignore    <- NULL
crs$weights   <- NULL

 

# The 'Hmisc' package provides the 'contents' function.

library(Hmisc, quietly=TRUE)

# Obtain a summary of the dat# The following variable selections have been noted.

crs$input     <- c("Age", "Female", "Income", "Married",
                   "Children", "Loan", "Mortgage")

crs$numeric   <- c("Age", "Female", "Income", "Married",
                   "Children", "Loan", "Mortgage")

crs$categoric <- NULL

crs$target    <- NULL
crs$risk      <- NULL
crs$ident     <- NULL
crs$ignore    <- NULL
crs$weights   <- NULL

contents(crs$dataset[, c(crs$input, crs$risk, crs$target)])
summary(crs$dataset[, c(crs$input, crs$risk, crs$target)])

# Generate a description of the dataset.

describe(crs$dataset[, c(crs$input, crs$risk, crs$target)])

```

```{r echo=FALSE,message=FALSE,warning=FALSE }
# The 'mice' package provides the 'md.pattern' function.

library(mice, quietly=TRUE)

# Generate a summary of the missing values in the dataset.

md.pattern(crs$dataset[,c(crs$input, crs$target)])
```

The mice function confirms that the data has no missing values.

#### 2.1.1 Age

After exploring the data,we can conclude that the minimum age of customers is 22 and maximum age is 66.The average age of customers is 45.97.

```{r echo=FALSE,message=FALSE,warning=FALSE}

# Display box plots for the selected variables. 

# Use ggplot2 to generate box plot for Age

# Generate a box plot.

p01 <- crs %>%
  with(dataset[,]) %>%
  ggplot2::ggplot(ggplot2::aes(y=Age)) +
  ggplot2::geom_boxplot(ggplot2::aes(x="All"), notch=TRUE, fill="grey") +
  ggplot2::stat_summary(ggplot2::aes(x="All"), fun.y=mean, geom="point", shape=8) +
  ggplot2::xlab("Rattle 2025-Jul-18 21:09:09 midhu") +
  ggplot2::ggtitle("Distribution of Age") +
  ggplot2::theme(legend.position="none")

# Display the plots.

gridExtra::grid.arrange(p01)





```

The box plot shows that there is no outliers or extreme values in the data.

```{r echo=FALSE,message=FALSE,warning=FALSE}
# Display histogram plots for the selected variables. 

# Use ggplot2 to generate histogram plot for Age

# Generate the plot.

p01 <- crs %>%
  with(dataset[,]) %>%
  dplyr::select(Age) %>%
  ggplot2::ggplot(ggplot2::aes(x=Age)) +
  ggplot2::geom_density(lty=3) +
  ggplot2::xlab("Age\n\nRattle 2025-Jul-18 21:09:10 midhu") +
  ggplot2::ggtitle("Distribution of Age") +
  ggplot2::labs(y="Density")

# Display the plots.

gridExtra::grid.arrange(p01)
```

The histogram shows that most people are in the age range between 40s and 60s.It means that it is a kind of bi-model figure.

#### 2.1.2 Income

After data exploration the company came to conclusion that that the income range of customers is between ₹8,877 and ₹59,804.The average value being ₹28,012 and median being ₹24,241.

Right-skewed (mean \> median)

```{r echo=FALSE,message=FALSE,warning=FALSE}
# Display box plots for the selected variables. 

# Use ggplot2 to generate box plot for Income

# Generate a box plot.

p01 <- crs %>%
  with(dataset[,]) %>%
  ggplot2::ggplot(ggplot2::aes(y=Income)) +
  ggplot2::geom_boxplot(ggplot2::aes(x="All"), notch=TRUE, fill="grey") +
  ggplot2::stat_summary(ggplot2::aes(x="All"), fun.y=mean, geom="point", shape=8) +
  ggplot2::xlab("Rattle 2025-Jul-18 21:12:10 midhu") +
  ggplot2::ggtitle("Distribution of Income") +
  ggplot2::theme(legend.position="none")

# Display the plots.

gridExtra::grid.arrange(p01)

```

***Box Plot Interpretation***

The box plot for income shows that there are no extreme values.

```{r echo=FALSE,message=FALSE,warning=FALSE}
# Display histogram plots for the selected variables. 

# Use ggplot2 to generate histogram plot for Income

# Generate the plot.

p01 <- crs %>%
  with(dataset[,]) %>%
  dplyr::select(Income) %>%
  ggplot2::ggplot(ggplot2::aes(x=Income)) +
  ggplot2::geom_density(lty=3) +
  ggplot2::xlab("Income\n\nRattle 2025-Jul-18 21:12:11 midhu") +
  ggplot2::ggtitle("Distribution of Income") +
  ggplot2::labs(y="Density")

# Display the plots.

gridExtra::grid.arrange(p01)
```

***Histogram Interpretation***

The histogram for income is positively skewed (skewed to the right) which shows that most people have low income and only very small number of people have very high income

#### 2.1.3 Gender

The data provides the information that majority (56.7%) are females as the mean is 0.5667 and median is 1.

#### 2.1.4 Marital Status

The data shows that 80% of the people are married as the mean is 0.8 and the remaining 20% are unmarried.

#### 2.1.5 Number of children

The data depicts that the maximum number of children for the customers in the given set is 3 per individual.However,on average there is only one child per individual as the mean is (0.933).As the median is 0.5,most customers have 0 child.

#### 2.1.6 Loan Status

Out of the total customers,nearly 44% has loan liability.Majority of the customers have no loan as the median is 0.

#### 2.1.7 Mortgage

The mortgage status is almost similar to loan status as only 40% have a mortgage as mean is 0.4 and majority does not have mortgage as median being 0.

## 3. **Segmentation using Clustering**

**Clustering** is a method of grouping the observations based on their similarities.We use distance measures for assessing the dissimilarity among the observations.

### 3.1 Hierarchical Clustering

**Hierarchical Clustering** is an algorithm that builds a hierarchy (tree) of clusters, without pre-specifying the number of clusters. It can be visualized using a **dendrogram**, which shows how clusters are merged or split over time.

**Hierarchical clustering** is performed with number of clusters as 10 and **dendrogram** is created

```{r echo=FALSE,message=FALSE,warning=FALSE}
# Hierarchical Cluster 

# Generate a hierarchical cluster from the numeric data.

crs$dataset[, crs$numeric] %>%
  amap::hclusterpar(method="euclidean", link="ward", nbproc=1) ->
crs$hclust

# Time taken: 0.04 secs
```

```{r echo=FALSE,message=FALSE,warning=FALSE}
# Dendrogram Plot 

# The 'ggplot2' package provides the 'ggplot' function.

library(ggplot2, quietly=TRUE)

# The 'ggdendro' package provides the 'dendro_data' function.

library(ggdendro, quietly=TRUE)

# Generate the dendrogram plot.

ddata <- dendro_data(crs$hclust, type="rectangle")
g <- ggplot(segment(ddata))
g <- g + geom_segment(aes(x = y, y = x, xend = yend, yend = xend))
g <- g + scale_y_discrete(labels = ddata$label$label)
g <- g + labs(x="Height", y="Observation")
g <- g + ggtitle(expression(atop("Cluster Dendrogram DemoKTC.xlsx", atop(italic("Rattle 2025-Jul-18 21:21:20 midhu")))))
print(g)
```

The dendrogram shows the agglomeration at different ages.After the hierarchical clustering is done,k means clustering is done with number of clusters set to 5 and later to 3 clusters.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(readxl)
crs$dataset <- read_excel("C:/Users/midhu/OneDrive/Desktop/R Prgm/DemoKTC.xlsx", guess_max=1e4)
mydata<-scale(crs$dataset)
d <- dist(mydata, method = "manhattan") # distance matrix
fit <- hclust(d, method="ward") # Clustering
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
#draw dendogram with red borders around the 5 clusters
rect.hclust(fit, k=3, border="red")
```

### 3.2 K-Means Clustering

**K-means** is an **unsupervised machine learning algorithm** used to partition a dataset into **K distinct, non-overlapping clusters** based on similarity.It organizes observations into similar groups.

```{r echo=FALSE,message=FALSE,warning=FALSE}
# KMeans 

# Reset the random number seed to obtain the same results each time.

set.seed(crv$seed)

# The 'reshape' package provides the 'rescaler' function.

library(reshape, quietly=TRUE)

# Generate a kmeans cluster of size 5.

crs$kmeans <- kmeans(sapply(na.omit(crs$dataset[, crs$numeric]), rescaler, "range"), 5)

#=======================================================================

# Report on the cluster characteristics. 

# Cluster sizes:

paste(crs$kmeans$size, collapse=' ')

# Data means:

colMeans(sapply(na.omit(crs$dataset[, crs$numeric]), rescaler, "range"))

# Cluster centers:

crs$kmeans$centers

# Within cluster sum of squares:

crs$kmeans$withinss

# Time taken: 0.00 secs
```

As the diagram shows too much overlapping between the variables,it is decided to change the cluster size to 3.

```{r echo=FALSE,message=FALSE,warning=FALSE}
# Report on the cluster characteristics. 

# Cluster sizes:

paste(crs$kmeans$size, collapse=' ')

# Data means:

colMeans(sapply(na.omit(crs$dataset[, crs$numeric]), rescaler, "range"))

# Cluster centers:

crs$kmeans$centers

# Within cluster sum of squares:

crs$kmeans$withinss

# Time taken: 0.00 secs

# Generate a discriminant coordinates plot.

cluster::clusplot(na.omit(crs$dataset[, intersect(crs$input, crs$numeric)]), crs$kmeans$cluster, color=TRUE, shade=TRUE, main='Discriminant Coordinates DemoKTC.xlsx')

```

As there is many overlapping when clustering is done using 5 clusters,the elbow diagram is prepared to know the optimal numbers of clusters to be used for the segmentation.

```{r echo=FALSE,message=FALSE,warning=FALSE}
library(factoextra)
library(cluster)
library(readxl, quietly=TRUE)
mydata <- crs$dataset
data <- scale(mydata)  
fviz_nbclust(data, kmeans, method = "wss")
set.seed(123)  # For reproducibility
km <- kmeans(data, centers = 3, nstart = 25)
set.seed(123)  # For reproducibility
km <- kmeans(data, centers = 3, nstart = 25)
fviz_cluster(km, data)
data2<-data # duplicating the data
data2$cluster<-km$cluster# writing the cluster membership in to the data
data2$cluster
```

After preparing the elbow plot the elbow or the curve is visible in the 3rd point,which means that the optimal cluster size is 3.So, using the K means clustering,the discriminant plot is prepared with 3 clusters.It is found from the graph that there is less overlapping compared to previous diagram.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Rattle timestamp: 2025-07-17 12:27:46.081723 x86_64-w64-mingw32 

# KMeans 

# Reset the random number seed to obtain the same results each time.

set.seed(crv$seed)

# The 'reshape' package provides the 'rescaler' function.

library(reshape, quietly=TRUE)

# Generate a kmeans cluster of size 3.

crs$kmeans <- kmeans(sapply(na.omit(crs$dataset[, crs$numeric]), rescaler, "range"), 3)



#4
# Rattle timestamp: 2025-07-17 12:27:46.252969 x86_64-w64-mingw32 

# Report on the cluster characteristics. 

# Cluster sizes:

paste(crs$kmeans$size, collapse=' ')

# Data means:

colMeans(sapply(na.omit(crs$dataset[, crs$numeric]), rescaler, "range"))

# Cluster centers:

crs$kmeans$centers

# Within cluster sum of squares:

crs$kmeans$withinss

# Time taken: 0.00 secs

# Generate a discriminant coordinates plot.

cluster::clusplot(na.omit(crs$dataset[, intersect(crs$input, crs$numeric)]), crs$kmeans$cluster, color=TRUE, shade=TRUE, main='Discriminant Coordinates DemoKTC.xlsx')


#5
data2<-mydata# duplicating the data
cluster_id<-as.vector(unlist(km$cluster))# writing the cluster membership in to the data
data2<-as.data.frame(cbind(data2,cluster_id))

# Group data2 by cluster_id and compute mean for each group
group_means <- aggregate(. ~ cluster_id, data = data2, FUN = mean)

# Split the original data into a list of data frames by cluster_id
grouped_data <- split(data2, data2$cluster_id)

# If we specifically want 3 data sets, we can extract them like this:
data_cluster1 <- grouped_data[[1]]
data_cluster2 <- grouped_data[[2]]
data_cluster3 <- grouped_data[[3]]

# Optionally view the group means
print(group_means)
```

This is the k means clustering with 3 clusters and discriminant plot is prepared.We can find that the overlapping is less with 3 clusters compared to clustering done with 5 clusters.
